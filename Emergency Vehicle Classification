# imports
import numpy as np
import os
import matplotlib.pyplot as plt
from PIL import Image, ImageOps, ImageFilter, ImageEnhance
import pandas as pd

# importing pytorch library.
import torch
import torchvision.transforms as transforms
import torch.nn.functional as F
import torch.nn as nn
from torch.utils.data import Dataset, random_split, DataLoader

# name of the image folder
imagePaths = '/content/drive/MyDrive/Capstone/images'

# reading the train.csv file using pandas
trainImages = pd.read_csv('/content/drive/MyDrive/Capstone/images/train.csv')
# reading the test.csv file using pandas
testImages = pd.read_csv('/content/drive/MyDrive/Capstone/images/test.csv')
# reading the submission file using pandas
samples = pd.read_csv('/content/drive/MyDrive/Capstone/images/sample_submission.csv')


# defining train and labels list to store images and labels respectively.
train = []
labels = []

for image, label in zip(trainImages.iloc[:, 0], trainImages.iloc[:, 1]):
	# create a image path and store in img_path variable
	imgPath = os.path.join(imagePaths, image)
	# Use PIl Image class to load the image
	img = Image.open(imgPath)

	# apply median filter to the image this helps in reducing noise
	img = img.filter(ImageFilter.MedianFilter)
	# convert the image to numpy array and store the loaded images into train
	train.append(np.asarray(img))
	# store the label into the labels list
	labels.append(label)
  
  
  # create subplots using the plt.subplots function
# the number of subplots depend on the n_rows and n_cols
# all the subplots are stored in ax variables
_, ax = plt.subplots(nrows = 4, ncols = 7, figsize =(12, 12))

# iterate through the ax variable by flattening it
for index, i in enumerate(ax.flatten()):
	# the imshow is used to show the image
	i.imshow(train[index])
	# set the title
	i.set_title(index)

	# this below lines makes the code better visualize.
	i.set_xticks([])
	i.set_yticks([])
  
  
  # Creating a VehicleDataset class for loading the images and labels .
# the following class needs to extend from the Dataset class
# provided by pytorch framework and implement the __len__ and __getitem__ methods.

class VehicleDataset(Dataset):

	def __init__(self, csv_name, folder, transform = None, label = False):

		self.label = label

		self.folder = folder
		print(csv_name)
		self.dataframe = pd.read_csv('/content/drive/MyDrive/Capstone'+'/' + self.folder+'/'+csv_name+'.csv')
		self.tms = transform

	def __len__(self):
		return len(self.dataframe)

	def __getitem__(self, index):

		row = self.dataframe.iloc[index]

		imgIndex = row['image_names']

		imageFile = self.folder + '/' + imgIndex

		image = Image.open(imageFile)

		if self.label:
			target = row['emergency_or_not']

			if target == 0:
				encode = torch.FloatTensor([1, 0])
			else:
				encode = torch.FloatTensor([0, 1])

			return self.tms(image), encode

		return self.tms(image)


# creating objects of VehicleDataset

# the deep learning models accepts the image to be in tensor format
# this is done using the transforms.ToTensor() methods

transform = transforms.Compose([transforms.ToTensor(),])

trainDataset = VehicleDataset('train', 'images', label = True, transform = transform)

class ImageClassificationBase(nn.Module):
    
    def training_step(self, batch):
        images, targets = batch 
        out = self(images)     
        # _,out = torch.max(out,dim = 1)                 
        loss = F.binary_cross_entropy(torch.sigmoid(out), targets)      
        return loss
    
    def validation_step(self, batch):
        images, targets = batch 
        out = self(images)      

                                                # Generate predictions
        loss = F.binary_cross_entropy(torch.sigmoid(out), targets) 
       
        score = accuracy(out, targets)
        return {'val_loss': loss.detach(),'val_score':score.detach()}
        
    #this 2 methods will not change .
    
    def validation_epoch_end(self, outputs):
        batch_losses = [x['val_loss'] for x in outputs]
        epoch_loss = torch.stack(batch_losses).mean()   # Combine losses
        batch_scores = [x['val_score'] for x in outputs]
        epoch_score = torch.stack(batch_scores).mean()      # Combine accuracies
        return {'val_loss': epoch_loss.item(), 'val_score': epoch_score.item()}
    
    def epoch_end(self, epoch, result):
        print("Epoch [{}], train_loss: {:.4f}, val_loss: {:.4f}, val_score: {:.4f}".format(
            epoch, result['train_loss'], result['val_loss'], result['val_score']))
            
            
   # the EmergencyCustomModel class defines our Neural Network
# It inherites from the ImageClassificationBase class which has helper methods
# for printing the loss and accuracy at each epochs.


class EmergencyCustomModel(ImageClassificationBase):
	def __init__(self):
		super().__init__()
		self.network = nn.Sequential(

			nn.Conv2d(3, 32, kernel_size = 3, padding = 1),
			nn.BatchNorm2d(32),
			nn.ReLU(),
			nn.MaxPool2d(2, 2),

			nn.Conv2d(32, 64, kernel_size = 3, stride = 1, padding = 1),
			nn.BatchNorm2d(64),
			nn.ReLU(),
			nn.MaxPool2d(2, 2),

			nn.Conv2d(64, 64, kernel_size = 3, stride = 1, padding = 1),
			nn.BatchNorm2d(64),
			nn.ReLU(),
			nn.MaxPool2d(2, 2),

			nn.Conv2d(64, 128, kernel_size = 3, stride = 1, padding = 1),
			nn.BatchNorm2d(128),
			nn.ReLU(),
			nn.MaxPool2d(2, 2),

			nn.Conv2d(128, 128, kernel_size = 3, stride = 1, padding = 1),
			nn.BatchNorm2d(128),
			nn.ReLU(),
			nn.MaxPool2d(2, 2),

			nn.Conv2d(128, 256, kernel_size = 3, stride = 1, padding = 1),
			nn.BatchNorm2d(256),
			nn.ReLU(),
			nn.AdaptiveAvgPool2d(1),

			nn.Flatten(),
			nn.Linear(256, 128),
			nn.ReLU(),
			nn.Linear(128, 64),
			nn.ReLU(),
			nn.Linear(64, 2),
			# nn.Sigmoid(),
		)
def forward(self, xb):
		return self.network(xb)


# defining the training method.
# the evaluation method is used to calculate validation accuracy.


torch.no_grad()
def evaluate(model, val_loader):
	model.eval()
	outputs = [model.validation_step(batch) for batch in val_loader]
	return model.validation_epoch_end(outputs)


# The fit method is used to train the model
# parameters
'''
epochs: no. of epochs the model trains
max_lr: maximum learning rate.
train_loader: here we pass the train dataset
val_loader: here we pass the val_dataset
opt_func : The learning algorithm that performs gradient descent.
model : the neural network to train on.
'''

def fit(epochs, max_lr, model, train_loader, val_loader,
		weight_decay = 0, grad_clip = None, opt_func = torch.optim.SGD):
	torch.cuda.empty_cache()
	history = []

	# Set up custom optimizer with weight decay
	optimizer = opt_func(model.parameters(), max_lr, weight_decay = weight_decay)

	# the loop iterates from 0 to number of epochs.
	# the model needs to be set in the train model by calling the model.train.

	for epoch in range(epochs):
		# Training Phase
		model.train()
		train_losses = []

		for batch in train_loader:
			loss = model.training_step(batch)
			train_losses.append(loss)
			loss.backward()

			# Gradient clipping
			if grad_clip:
				nn.utils.clip_grad_value_(model.parameters(), grad_clip)

			optimizer.step()
			optimizer.zero_grad()

		# Validation phase
		result = evaluate(model, val_loader)
		result['train_loss'] = torch.stack(train_losses).mean().item()
		model.epoch_end(epoch, result)
		history.append(result)
	return history


# the batchSize is the number of images passes by the loader at a time.
# reduce this number if theres an out of memory error.
batchSize = 32
valPct = 0.2

# code for splitting the data
# valPct variable is used to split dataset

valSize = int(valPct * len(trainDataset))
trainSize = len(trainDataset) - valSize
trainDs, valDs = random_split(trainDataset, [trainSize, valSize])

# Creating dataloaders.
train_loader = DataLoader(trainDs, batchSize)
val_loader = DataLoader(valDs, batchSize)


customModel = EmergencyCustomModel()
epochs = 10
lr = 0.01

# save the history to visualize later.
history = fit(epochs, lr, customModel, trainDs, valDs)
